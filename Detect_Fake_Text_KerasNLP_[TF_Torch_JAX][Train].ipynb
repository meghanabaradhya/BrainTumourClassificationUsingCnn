{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 61542,
          "databundleVersionId": 6888007,
          "sourceType": "competition"
        },
        {
          "sourceId": 6847931,
          "sourceType": "datasetVersion",
          "datasetId": 3936750
        },
        {
          "sourceId": 6867914,
          "sourceType": "datasetVersion",
          "datasetId": 3946973
        },
        {
          "sourceId": 6890527,
          "sourceType": "datasetVersion",
          "datasetId": 3942644
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghanabaradhya/BrainTumourClassificationUsingCnn/blob/main/Detect_Fake_Text_KerasNLP_%5BTF_Torch_JAX%5D%5BTrain%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'daigt-external-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3936750%2F6847931%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T070236Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db147652a2d6a1d90298c674b183e87bca6c10007c8f9295b3ae0a4522e1987c22b0a0605fd09a41bed178fffa6131e14c4ae6dee1e6b1eb4ee2bd5c0c6876fddf96db7707d64eee25cd646474f272ac2d19b07ce21dd80b256b88063f4c242df8111e017ab9e24b4d3a7e627abb7c3f6da167f3508c2a17d9342d18f77fad34cfb0f4d02986f91552e14d9291c9d895578f7aa6506d9ec0bcfa1caa7f6938747f569a127b0d1197abb6cfea76a95db16305f03133d7ecb9f981c0e6862ff73a8327a6995492a8a65379812680677c67a13dfa891b1b02a8547b2ff61e3f01f17786c11e1b784a4f2dd9800ede9dc95af48f5e49ad2ef716b1b4a3e0921e48051,argugpt:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3946973%2F6867914%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T070237Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2d9240a474f6efc2a10ba6ae37b982b64897b221dcf829d632d2935f0e9534409d0f72be31f476f2bdb9f6df959ade57ad955ab111b3f6ebecc7a6ed16392e8ee9f28ea05549adb1f8b22830843b92e55ce125bc0a032676584ea2284cc27fc15065954ceb80cd540244d1b672655e1dc650a5c37a2cce7760f2912c22646523bcaa593b9a99dc923ddb55436cef2f38023775212ff3b605fc3b8a53dc3b47efcefb43af2ecf96384e9ec9c7f9d8cb70df549fef4318386a433866c66254c54e1dd15c9abee88c5c17b3e69fc9814578bac43c560b865b04af812e0a5a83e58fb27489c3a50c6acab3366129ad9b61d98031890105e6489885fcd4340a7b8858,daigt-proper-train-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3942644%2F6890527%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240412%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240412T070237Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da9855088d5d5acaf6a1822526e8032f6ea0a79250e0f9203efcd78199f191c56c56166fcd4e3a93ad8069790dc18ca2a5a67e24c62cbcbfe57c2208e830ee0ca7d446aeb69da473d98ead2537a9bd2aec1385095cd5e598cf7198524b06c3adcd56c6b1f8a4289821224e146694745a209104223583ee42b3818f52826b29392d262c6d9ce55996be035568fc3e1f3f5d1b30be8ca4257a1ef0842f7454b431104362e95a4012b8dc66a48a3349ebfffbbd42553c1a05b654ab9d7a4ca9ca79d3c6d6a6fb06dc25a70c56afa63cb67d1977560619e95d5e3cefee8678741a5eea8332a0570f8c73485316dd2b454bb817096ddf2f17505160376a58900ef438b'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "nWFX2hYmHZOx"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM - Detect AI Generated Text\n",
        "> Identify which essay was written by a large language model\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/36858976/279902422-b365f6ef-ef01-49ac-af7f-0bc2ca3ba835.png\">"
      ],
      "metadata": {
        "id": "ghTnuILvHZO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎯 | Motivation\n",
        "\n",
        "* In this notebook, we will demonstrate the usage of the multi-backend capabilities of `KerasCore` and `KerasNLP` for the **Detecting Fake Text** task.\n",
        "* Thanks to `KerasCore`, soon to become `Keras 3.0`, which enables seamless execution of this notebook on `TensorFlow`, `Jax`, and `PyTorch` platforms with minimal adjustments required.\n",
        "* Moreover, this notebook supports both single/multi GPU and TPU training. As time progresses, larger datasets may become available, making TPUs invaluable for training substantial models on these extensive datasets.\n",
        "* Finally, I am very curious about the effectiveness of this competition, as a few months ago, `OpenAI` shut down their AI Text-Detection Tool due to inaccuracies ([ref](https://www.pcmag.com/news/openai-quietly-shuts-down-ai-text-detection-tool-over-inaccuracies)). So, I'm eager to see how well this competition will address the problem.\n",
        "<img src=\"https://i.ibb.co/6Y2Vtgr/openAI.jpg\" alt=\"openAI\" border=\"0\">"
      ],
      "metadata": {
        "id": "cApwh-YlHZO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📓 | Notebooks\n",
        "\n",
        "* Train: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Train]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-train)\n",
        "* Infer: [Detect Fake Text: KerasNLP [TF/Torch/JAX][Infer]](https://www.kaggle.com/code/awsaf49/detect-fake-text-kerasnlp-tf-torch-jax-infer)"
      ],
      "metadata": {
        "id": "lQ4AlxBBHZO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠 | Install Libraries"
      ],
      "metadata": {
        "id": "8WXoisiSHZO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras_nlp==0.6.3 keras-core==0.1.7"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-17T07:02:12.726262Z",
          "iopub.execute_input": "2024-01-17T07:02:12.727011Z",
          "iopub.status.idle": "2024-01-17T07:02:21.003355Z",
          "shell.execute_reply.started": "2024-01-17T07:02:12.726973Z",
          "shell.execute_reply": "2024-01-17T07:02:21.002547Z"
        },
        "trusted": true,
        "id": "kdGrrcJAHZO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 | Import Libraries"
      ],
      "metadata": {
        "id": "0OiOhnojHZO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # \"jax\" or \"tensorflow\" or \"torch\"\n",
        "# os.environ[\"WANDB_SILENT\"] = \"false\" # for wandb\n",
        "\n",
        "import keras_nlp\n",
        "import keras_core as keras\n",
        "import keras_core.backend as K\n",
        "\n",
        "\n",
        "import torch\n",
        "# import jax\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# import tensorflow.keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "cmap = mpl.cm.get_cmap('coolwarm')"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:16.42969Z",
          "iopub.execute_input": "2024-01-17T07:08:16.430037Z",
          "iopub.status.idle": "2024-01-17T07:08:22.480926Z",
          "shell.execute_reply.started": "2024-01-17T07:08:16.430006Z",
          "shell.execute_reply": "2024-01-17T07:08:22.480019Z"
        },
        "trusted": true,
        "id": "tUYmbAy-HZO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Version"
      ],
      "metadata": {
        "id": "95ZV_G5cHZO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TensorFlow:\", tf.__version__)\n",
        "# print(\"JAX:\", jax.__version__)\n",
        "print(\"Keras:\", keras.__version__)\n",
        "print(\"KerasNLP:\", keras_nlp.__version__)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:22.4823Z",
          "iopub.execute_input": "2024-01-17T07:08:22.482804Z",
          "iopub.status.idle": "2024-01-17T07:08:26.812802Z",
          "shell.execute_reply.started": "2024-01-17T07:08:22.482773Z",
          "shell.execute_reply": "2024-01-17T07:08:26.811913Z"
        },
        "trusted": true,
        "id": "3UxRN1fnHZO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚙️ | Configuration"
      ],
      "metadata": {
        "id": "X0ffn3VwHZO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    verbose = 0  # Verbosity\n",
        "\n",
        "    wandb = True  # Weights & Biases logging\n",
        "    competition = 'llm-detect-ai-generated-text'  # Competition name\n",
        "    _wandb_kernel = 'awsaf49'  # WandB kernel\n",
        "    comment = 'DebertaV3-MaxSeq_200-ext_s-torch'  # Comment description\n",
        "\n",
        "    preset = \"deberta_v3_base_en\"  # Name of pretrained models\n",
        "    sequence_length = 200  # Input sequence length\n",
        "\n",
        "    device = 'TPU'  # Device\n",
        "\n",
        "    seed = 42  # Random seed\n",
        "\n",
        "    num_folds = 5  # Total folds\n",
        "    selected_folds = [0, 1]  # Folds to train on\n",
        "\n",
        "    epochs = 3 # Training epochs\n",
        "    batch_size = 3  # Batch size\n",
        "    drop_remainder = True  # Drop incomplete batches\n",
        "    cache = True # Caches data after one iteration, use only with `TPU` to avoid OOM\n",
        "\n",
        "    scheduler = 'cosine'  # Learning rate scheduler\n",
        "\n",
        "    class_names = [\"real\", \"fake\"]  # Class names [A, B, C, D, E]\n",
        "    num_classes = len(class_names)  # Number of classes\n",
        "    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]\n",
        "    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping\n",
        "    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:26.814618Z",
          "iopub.execute_input": "2024-01-17T07:08:26.814933Z",
          "iopub.status.idle": "2024-01-17T07:08:26.945168Z",
          "shell.execute_reply.started": "2024-01-17T07:08:26.814902Z",
          "shell.execute_reply": "2024-01-17T07:08:26.943901Z"
        },
        "trusted": true,
        "id": "sFSOaFfHHZO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ♻️ | Reproducibility\n",
        "Sets value for random seed to produce similar result in each run."
      ],
      "metadata": {
        "id": "gPW3nO5xHZO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(CFG.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:26.946344Z",
          "iopub.execute_input": "2024-01-17T07:08:26.946609Z",
          "iopub.status.idle": "2024-01-17T07:08:27.149418Z",
          "shell.execute_reply.started": "2024-01-17T07:08:26.946581Z",
          "shell.execute_reply": "2024-01-17T07:08:27.14855Z"
        },
        "trusted": true,
        "id": "8z-YZafYHZO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💾 | Hardware\n",
        "Following codes automatically detects hardware (TPU or GPU)."
      ],
      "metadata": {
        "id": "DjWfGt3IHZO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"Detect and intializes GPU/TPU automatically\"\n",
        "    try:\n",
        "        # detect and init the TPU\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "        # instantiate a distribution strategy\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        print(f'> Running on TPU', tpu.master(), end=' | ')\n",
        "        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n",
        "        device=CFG.device\n",
        "    except:\n",
        "        # If TPU is not available, detect GPUs\n",
        "        gpus = tf.config.list_logical_devices('GPU')\n",
        "        ngpu = len(gpus)\n",
        "         # Check number of GPUs\n",
        "        if ngpu:\n",
        "            # Set GPU strategy\n",
        "            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n",
        "            # Print GPU details\n",
        "            print(\"> Running on GPU\", end=' | ')\n",
        "            print(\"Num of GPUs: \", ngpu)\n",
        "            device='GPU'\n",
        "        else:\n",
        "            # If no GPUs are available, use CPU\n",
        "            print(\"> Running on CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            device='CPU'\n",
        "    return strategy, device"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:27.15101Z",
          "iopub.execute_input": "2024-01-17T07:08:27.151285Z",
          "iopub.status.idle": "2024-01-17T07:08:27.300454Z",
          "shell.execute_reply.started": "2024-01-17T07:08:27.151257Z",
          "shell.execute_reply": "2024-01-17T07:08:27.299763Z"
        },
        "trusted": true,
        "id": "BwinudukHZO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GPU/TPU/TPU-VM\n",
        "strategy, CFG.device = get_device()\n",
        "CFG.replicas = strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:08:27.301408Z",
          "iopub.execute_input": "2024-01-17T07:08:27.301724Z",
          "iopub.status.idle": "2024-01-17T07:08:35.564984Z",
          "shell.execute_reply.started": "2024-01-17T07:08:27.301683Z",
          "shell.execute_reply": "2024-01-17T07:08:35.564234Z"
        },
        "trusted": true,
        "id": "zcKyNYs9HZO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📁 | Dataset Path"
      ],
      "metadata": {
        "id": "krZUClyBHZO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/kaggle/input/llm-detect-ai-generated-text'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:20.375299Z",
          "iopub.execute_input": "2024-01-17T07:09:20.376098Z",
          "iopub.status.idle": "2024-01-17T07:09:20.379407Z",
          "shell.execute_reply.started": "2024-01-17T07:09:20.376059Z",
          "shell.execute_reply": "2024-01-17T07:09:20.37867Z"
        },
        "trusted": true,
        "id": "qxqkAlh4HZO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 | Meta Data\n",
        "* `{test|train}_essays.csv`\n",
        "    * `id` - A unique identifier for each essay.\n",
        "    * `prompt_id` - Identifies the prompt the essay was written in response to.\n",
        "    * `text` - The essay text itself.\n",
        "    * `generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n",
        "* **sample_submission.csv** - is the valid sample submission."
      ],
      "metadata": {
        "id": "xgnRscmZHZO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Data"
      ],
      "metadata": {
        "id": "Fe_hsSCQHZO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'{BASE_PATH}/train_essays.csv')  # Read CSV file into a DataFrame\n",
        "df['label'] = df.generated.copy()\n",
        "df['name'] = df.generated.map(CFG.label2name)  # Map answer labels using name-to-label mapping\n",
        "\n",
        "# Display information about the train data\n",
        "print(\"# Train Data: {:,}\".format(len(df)))\n",
        "print(\"# Sample:\")\n",
        "display(df.head(2))\n",
        "\n",
        "# Show distribution of answers using a bar plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "df.name.value_counts().plot.bar(color=[cmap(0.0), cmap(0.25), cmap(0.65), cmap(0.9), cmap(1.0)])\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Class distribution for Train Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:21.502773Z",
          "iopub.execute_input": "2024-01-17T07:09:21.503678Z",
          "iopub.status.idle": "2024-01-17T07:09:21.806509Z",
          "shell.execute_reply.started": "2024-01-17T07:09:21.503635Z",
          "shell.execute_reply": "2024-01-17T07:09:21.805766Z"
        },
        "trusted": true,
        "id": "XJmypNPNHZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## External Datasets\n",
        "\n",
        "We'll be utilizing the following external datasets:\n",
        "\n",
        "* [Proper Train Dataset](https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset/) @thedrcat\n",
        "* [ArguGPT](https://www.kaggle.com/datasets/alejopaullier/argugpt) @alejopaullier\n"
      ],
      "metadata": {
        "id": "Ha7OtLS5HZO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load external data\n",
        "ext_df1 = pd.read_csv('/kaggle/input/daigt-proper-train-dataset/train_drcat_04.csv')\n",
        "ext_df2 = pd.read_csv('/kaggle/input/argugpt/argugpt.csv')[['id','text','model']]\n",
        "\n",
        "ext_df2.rename(columns={'model':'source'}, inplace=True)\n",
        "ext_df2['label'] = 1\n",
        "\n",
        "ext_df = pd.concat([\n",
        "    ext_df1[ext_df1.source=='persuade_corpus'].sample(10000),\n",
        "    ext_df1[ext_df1.source!='persuade_corpus'],\n",
        "#     ext_df2,\n",
        "])\n",
        "\n",
        "# ext_real_df = ext_df[['id', 'text']].copy()\n",
        "# ext_real_df['label']  = 0\n",
        "\n",
        "# ext_fake_df = ext_df[['id', 'source_text']].copy()\n",
        "# ext_fake_df.rename(columns={\"source_text\":\"text\"}, inplace=True)\n",
        "# ext_fake_df['label']  = 1\n",
        "\n",
        "# ext_df = pd.concat([ext_real_df, ext_fake_df], axis=0)\n",
        "ext_df['name'] = ext_df.label.map(CFG.label2name)\n",
        "\n",
        "# Display information about the external data\n",
        "print(\"# External Data: {:,}\".format(len(ext_df)))\n",
        "print(\"# Sample:\")\n",
        "ext_df.head(2)\n",
        "\n",
        "# Show distribution of answers using a bar plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "ext_df.name.value_counts().plot.bar(color=[cmap(0.0), cmap(0.65)])\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Answer distribution for External Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:23.719878Z",
          "iopub.execute_input": "2024-01-17T07:09:23.720757Z",
          "iopub.status.idle": "2024-01-17T07:09:26.381951Z",
          "shell.execute_reply.started": "2024-01-17T07:09:23.720697Z",
          "shell.execute_reply": "2024-01-17T07:09:26.381205Z"
        },
        "trusted": true,
        "id": "wME_9x4RHZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine External and Train Data"
      ],
      "metadata": {
        "id": "yo6Ru9BCHZPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = ext_df.copy().reset_index(drop=True) # pd.concat([ext_df, df], axis=0)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:26.383385Z",
          "iopub.execute_input": "2024-01-17T07:09:26.383729Z",
          "iopub.status.idle": "2024-01-17T07:09:26.400923Z",
          "shell.execute_reply.started": "2024-01-17T07:09:26.383677Z",
          "shell.execute_reply": "2024-01-17T07:09:26.400091Z"
        },
        "trusted": true,
        "id": "nckjFGB9HZPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔪 | Data Split\n",
        "\n",
        "In the code snippet provided below, we will divide the existing **train** data into folds using a stratification of `label` column."
      ],
      "metadata": {
        "id": "5DvcKzePHZPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold  # Import package\n",
        "\n",
        "skf = StratifiedKFold(n_splits=CFG.num_folds, shuffle=True, random_state=CFG.seed)  # Initialize K-Fold\n",
        "\n",
        "df = df.reset_index(drop=True)  # Reset dataframe index\n",
        "\n",
        "df['stratify'] = df.label.astype(str)+df.source.astype(str)\n",
        "\n",
        "df[\"fold\"] = -1  # New 'fold' column\n",
        "\n",
        "# Assign folds using StratifiedKFold\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['stratify'])):\n",
        "    df.loc[val_idx, 'fold'] = fold\n",
        "\n",
        "# Display label distribution for each fold\n",
        "df.groupby([\"fold\", \"name\", \"source\"]).size()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:26.463429Z",
          "iopub.execute_input": "2024-01-17T07:09:26.463743Z",
          "iopub.status.idle": "2024-01-17T07:09:26.990864Z",
          "shell.execute_reply.started": "2024-01-17T07:09:26.463697Z",
          "shell.execute_reply": "2024-01-17T07:09:26.990104Z"
        },
        "trusted": true,
        "id": "-lHWLjnfHZPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍽️ | Preprocessing\n",
        "\n",
        "**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.\n",
        "\n",
        "**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `\"The quick brown fox\"` into `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.\n",
        "\n",
        "Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n",
        "- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)\n",
        "- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)"
      ],
      "metadata": {
        "id": "jp_eurCGHZPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
        "    preset=CFG.preset, # Name of the model\n",
        "    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:28.06239Z",
          "iopub.execute_input": "2024-01-17T07:09:28.063492Z",
          "iopub.status.idle": "2024-01-17T07:09:28.784042Z",
          "shell.execute_reply.started": "2024-01-17T07:09:28.063447Z",
          "shell.execute_reply": "2024-01-17T07:09:28.782894Z"
        },
        "trusted": true,
        "id": "5GMmS0YeHZPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\\_choices, sequence\\_length)$."
      ],
      "metadata": {
        "id": "G1GwV013HZPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = preprocessor(df.text.iloc[0])  # Process text for the first row\n",
        "\n",
        "# Display the shape of each processed output\n",
        "for k, v in inp.items():\n",
        "    print(k, \":\", v.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T07:09:29.860497Z",
          "iopub.execute_input": "2024-01-17T07:09:29.860854Z",
          "iopub.status.idle": "2024-01-17T07:09:34.304815Z",
          "shell.execute_reply.started": "2024-01-17T07:09:29.860821Z",
          "shell.execute_reply": "2024-01-17T07:09:34.303087Z"
        },
        "trusted": true,
        "id": "LPhdHd9CHZPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method."
      ],
      "metadata": {
        "id": "wa5m2j9-HZPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(text, label=None):\n",
        "    text = preprocessor(text)  # Preprocess text\n",
        "    return (text, label) if label is not None else text  # Return processed text and label if available"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:05.444966Z",
          "iopub.execute_input": "2024-01-17T06:52:05.445239Z",
          "iopub.status.idle": "2024-01-17T06:52:05.450385Z",
          "shell.execute_reply.started": "2024-01-17T06:52:05.445215Z",
          "shell.execute_reply": "2024-01-17T06:52:05.449329Z"
        },
        "trusted": true,
        "id": "ioTbL8Y3HZPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🍚 | DataLoader\n",
        "\n",
        "The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.\n",
        "\n",
        "To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data)."
      ],
      "metadata": {
        "id": "VSS_QGw1HZPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(texts, labels=None, batch_size=32,\n",
        "                  cache=False, drop_remainder=True,\n",
        "                  repeat=False, shuffle=1024):\n",
        "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
        "    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n",
        "    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n",
        "    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n",
        "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n",
        "    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n",
        "    opt = tf.data.Options()  # Create dataset options\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n",
        "        opt.experimental_deterministic = False\n",
        "    ds = ds.with_options(opt)  # Set dataset options\n",
        "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n",
        "    ds = ds.prefetch(AUTO)  # Prefetch next batch\n",
        "    return ds  # Return the built dataset"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:05.453167Z",
          "iopub.execute_input": "2024-01-17T06:52:05.453456Z",
          "iopub.status.idle": "2024-01-17T06:52:05.462519Z",
          "shell.execute_reply.started": "2024-01-17T06:52:05.453425Z",
          "shell.execute_reply": "2024-01-17T06:52:05.461592Z"
        },
        "trusted": true,
        "id": "vaSpCYjZHZPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch Train/Valid Dataset\n",
        "\n",
        "The function below generates the training and validation datasets for a given fold."
      ],
      "metadata": {
        "id": "ooATd0gVHZPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_datasets(fold):\n",
        "    train_df = df[df.fold!=fold].sample(frac=1)  # Get training fold data\n",
        "\n",
        "    train_texts = train_df.text.tolist()  # Extract training texts\n",
        "    train_labels = train_df.label.tolist()  # Extract training labels\n",
        "\n",
        "    # Build training dataset\n",
        "    train_ds = build_dataset(train_texts, train_labels,\n",
        "                             batch_size=CFG.batch_size*CFG.replicas, cache=CFG.cache,\n",
        "                             shuffle=True, drop_remainder=True, repeat=True)\n",
        "\n",
        "    valid_df = df[df.fold==fold].sample(frac=1)  # Get validation fold data\n",
        "    valid_texts = valid_df.text.tolist()  # Extract validation texts\n",
        "    valid_labels = valid_df.label.tolist()  # Extract validation labels\n",
        "\n",
        "    # Build validation dataset\n",
        "    valid_ds = build_dataset(valid_texts, valid_labels,\n",
        "                             batch_size=min(CFG.batch_size*CFG.replicas, len(valid_df)), cache=CFG.cache,\n",
        "                             shuffle=False, drop_remainder=True, repeat=False)\n",
        "\n",
        "    return (train_ds, train_df), (valid_ds, valid_df)  # Return datasets and dataframes"
      ],
      "metadata": {
        "_kg_hide-input": false,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:05.46354Z",
          "iopub.execute_input": "2024-01-17T06:52:05.463927Z",
          "iopub.status.idle": "2024-01-17T06:52:05.474622Z",
          "shell.execute_reply.started": "2024-01-17T06:52:05.463893Z",
          "shell.execute_reply": "2024-01-17T06:52:05.4736Z"
        },
        "trusted": true,
        "id": "VoE81vqVHZPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🪄 | Wandb\n",
        "<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "To monitor the training of my text-based model, I'll make use of **Weights & Biases**. Weights & Biases (W&B) is an MLOps platform that offers experiment tracking, dataset versioning, and model management functionalities, aiding in efficient model development."
      ],
      "metadata": {
        "id": "pxd2gBaDHZPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb  # Import wandb library for experiment tracking\n",
        "\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient  # Import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()  # Create secrets client instance\n",
        "    api_key = user_secrets.get_secret(\"WANDB\")  # Get API key from Kaggle secrets\n",
        "    wandb.login(key=api_key)  # Login to wandb with the API key\n",
        "    anonymous = None  # Set anonymous mode to None\n",
        "except:\n",
        "    anonymous = 'must'  # Set anonymous mode to 'must'\n",
        "    wandb.login(anonymous=anonymous, relogin=True)  # Login to wandb anonymously and relogin if needed"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:05.475882Z",
          "iopub.execute_input": "2024-01-17T06:52:05.476257Z",
          "iopub.status.idle": "2024-01-17T06:52:08.64112Z",
          "shell.execute_reply.started": "2024-01-17T06:52:05.476232Z",
          "shell.execute_reply": "2024-01-17T06:52:08.640275Z"
        },
        "trusted": true,
        "id": "P6og7sFoHZPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logger\n",
        "\n",
        "The following code cell contains code to log data to WandB. It is noteworthy that the newly released callbacks offer more flexibility in terms of customization, and they are more compact compared to the classic `WandbCallback`, making it easier to use. Here's a brief introduction to them:\n",
        "\n",
        "* **WandbModelCheckpoint**: This callback saves the model or weights using `tf.keras.callbacks.ModelCheckpoint`. Hence, we can harness the power of the official TensorFlow callback to log even `tf.keras.Model` subclass model in TPU.\n",
        "* **WandbMetricsLogger**: This callback simply logs all the metrics and losses.\n",
        "* **WandbEvalCallback**: This one is even more special. We can use it to log the model's prediction after a certain epoch/frequency. We can use it to save segmentation masks, bounding boxes, GradCAM within epochs to check intermediate results and so on.\n",
        "\n",
        "For more details, please check the [official documentation](https://docs.wandb.ai/ref/python/integrations/keras)."
      ],
      "metadata": {
        "id": "IayfPKdvHZPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes the W&B run with a config file and W&B run settings.\n",
        "def wandb_init(fold):\n",
        "    config = {k: v for k, v in dict(vars(CFG)).items() if '__' not in k}  # Create config dictionary\n",
        "    config.update({\"fold\": int(fold)})  # Add fold to config\n",
        "    run = wandb.init(project=\"llm-fake-text\",\n",
        "                     name=f\"fold-{fold}|max_seq-{CFG.sequence_length}|model-{CFG.preset}\",\n",
        "                     config=config,\n",
        "                     group=CFG.comment,\n",
        "                     save_code=True)\n",
        "    return run\n",
        "\n",
        "# Log best result for error analysis\n",
        "def log_wandb():\n",
        "    wandb.log({'best_auc': best_auc, 'best_loss': best_loss, 'best_epoch': best_epoch})\n",
        "\n",
        "# Fetch W&B callbacks\n",
        "def get_wb_callbacks(fold):\n",
        "    wb_metr = wandb.keras.WandbMetricsLogger()\n",
        "    return [wb_metr]  # Return WandB callbacks"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.642188Z",
          "iopub.execute_input": "2024-01-17T06:52:08.642905Z",
          "iopub.status.idle": "2024-01-17T06:52:08.650953Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.642865Z",
          "shell.execute_reply": "2024-01-17T06:52:08.64999Z"
        },
        "trusted": true,
        "id": "eHt8BoZFHZPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚓ | LR Schedule\n",
        "\n",
        "Implementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:\n",
        "- `step`: Lowering the learning rate in step-wise manner resembling stairs.\n",
        "- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.\n",
        "- `exp`: Exponentially decreasing the learning rate.\n",
        "\n",
        "**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation."
      ],
      "metadata": {
        "id": "N9wfjG3YHZPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
        "    lr_start, lr_max, lr_min = 0.6e-6, 0.5e-6 * batch_size, 0.3e-6\n",
        "    lr_ramp_ep, lr_sus_ep, lr_decay = 1, 0, 0.75\n",
        "\n",
        "    def lrfn(epoch):  # Learning rate update function\n",
        "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
        "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
        "        elif mode == 'cos':\n",
        "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
        "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
        "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
        "        return lr\n",
        "\n",
        "    if plot:  # Plot lr curve if plot is True\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
        "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
        "        plt.title('LR Scheduler')\n",
        "        plt.show()\n",
        "\n",
        "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.652535Z",
          "iopub.execute_input": "2024-01-17T06:52:08.652868Z",
          "iopub.status.idle": "2024-01-17T06:52:08.66698Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.65284Z",
          "shell.execute_reply": "2024-01-17T06:52:08.666148Z"
        },
        "trusted": true,
        "id": "yENUI9c2HZPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_=get_lr_callback(CFG.batch_size*CFG.replicas, plot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.668019Z",
          "iopub.execute_input": "2024-01-17T06:52:08.668367Z",
          "iopub.status.idle": "2024-01-17T06:52:08.899673Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.668334Z",
          "shell.execute_reply": "2024-01-17T06:52:08.898597Z"
        },
        "trusted": true,
        "id": "UjuAH6_XHZPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ☎️ | Callbacks\n",
        "\n",
        "The function below will gather all the training callbacks, such as `lr_scheduler`, `model_checkpoint`, `wandb_logger`, and etc."
      ],
      "metadata": {
        "id": "S4EuPC0aHZPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_callbacks(fold):\n",
        "    callbacks = []\n",
        "    lr_cb = get_lr_callback(CFG.batch_size*CFG.replicas)  # Get lr callback\n",
        "    ckpt_cb = keras.callbacks.ModelCheckpoint(f'fold{fold}.keras',\n",
        "                                              monitor='val_auc',\n",
        "                                              save_best_only=True,\n",
        "                                              save_weights_only=False,\n",
        "                                              mode='max')  # Get Model checkpoint callback\n",
        "    callbacks.extend([lr_cb, ckpt_cb])  # Add lr and checkpoint callbacks\n",
        "\n",
        "    if CFG.wandb:  # If WandB is enabled\n",
        "        wb_cbs = get_wb_callbacks(fold)  # Get WandB callbacks\n",
        "        callbacks.extend(wb_cbs)\n",
        "\n",
        "    return callbacks  # Return the list of callbacks"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.900999Z",
          "iopub.execute_input": "2024-01-17T06:52:08.901334Z",
          "iopub.status.idle": "2024-01-17T06:52:08.908105Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.901305Z",
          "shell.execute_reply": "2024-01-17T06:52:08.907013Z"
        },
        "trusted": true,
        "id": "9Tp_kp91HZPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 | Modeling\n",
        "\n"
      ],
      "metadata": {
        "id": "xzmGAUSUHZPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KerasNLP Classifier\n",
        "\n",
        "<img src=\"https://keras.io/img/logo.png\" width=\"400\">\n",
        "\n",
        "The `KerasNLP` library provides comprehensive, ready-to-use implementations of popular NLP model architectures. It features a variety of pre-trained models including `Bert`, `Roberta`, `DebertaV3`, and more. In this notebook, we'll showcase the usage of `DebertaV3`. However, feel free to explore all available models in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). Also for a deeper understanding of `KerasNLP`, refer to the informative [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).\n",
        "\n",
        "Our approach involves using `keras_nlp.models.XXClassifier` to process each text and generatie logits. These logits are passed through a `softmax` function to produce the final output."
      ],
      "metadata": {
        "id": "u3GJEhRzHZPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    # Create a DebertaV3Classifier model\n",
        "    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n",
        "        CFG.preset,\n",
        "        preprocessor=None,\n",
        "        num_classes=1 # one output per one option, for five options total 5 outputs\n",
        "    )\n",
        "    inputs = classifier.input\n",
        "    logits = classifier(inputs)\n",
        "\n",
        "    # Compute final output\n",
        "    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model with optimizer, loss, and metrics\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.AdamW(5e-6),\n",
        "        loss=keras.losses.BinaryCrossentropy(label_smoothing=0.02),\n",
        "        metrics=[\n",
        "            keras.metrics.AUC(name=\"auc\"),\n",
        "        ],\n",
        "        jit_compile=True\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.909467Z",
          "iopub.execute_input": "2024-01-17T06:52:08.909853Z",
          "iopub.status.idle": "2024-01-17T06:52:08.923775Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.909809Z",
          "shell.execute_reply": "2024-01-17T06:52:08.922852Z"
        },
        "trusted": true,
        "id": "wN01Fk83HZPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with strategy.scope\n",
        "model = build_model()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:08.924847Z",
          "iopub.execute_input": "2024-01-17T06:52:08.925114Z",
          "iopub.status.idle": "2024-01-17T06:52:15.890636Z",
          "shell.execute_reply.started": "2024-01-17T06:52:08.92509Z",
          "shell.execute_reply": "2024-01-17T06:52:15.889589Z"
        },
        "trusted": true,
        "id": "oeEvdPyzHZPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Summary"
      ],
      "metadata": {
        "id": "KbJz0u2qHZPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:15.892028Z",
          "iopub.execute_input": "2024-01-17T06:52:15.892435Z",
          "iopub.status.idle": "2024-01-17T06:52:15.918866Z",
          "shell.execute_reply.started": "2024-01-17T06:52:15.8924Z",
          "shell.execute_reply": "2024-01-17T06:52:15.917944Z"
        },
        "trusted": true,
        "id": "mCVo7kquHZPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Plot"
      ],
      "metadata": {
        "id": "Gp3t3OtlHZPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:15.920115Z",
          "iopub.execute_input": "2024-01-17T06:52:15.920478Z",
          "iopub.status.idle": "2024-01-17T06:52:16.228512Z",
          "shell.execute_reply.started": "2024-01-17T06:52:15.920445Z",
          "shell.execute_reply": "2024-01-17T06:52:16.227525Z"
        },
        "trusted": true,
        "id": "WhgTLGJ6HZPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚂 | Training"
      ],
      "metadata": {
        "id": "b2VOr1oVHZPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in CFG.selected_folds:\n",
        "    # Initialize Weights and Biases if enabled\n",
        "    if CFG.wandb:\n",
        "        run = wandb_init(fold)\n",
        "\n",
        "    # Get train and validation datasets\n",
        "    (train_ds, train_df), (valid_ds, valid_df) = get_datasets(fold)\n",
        "\n",
        "    # Get callback functions for training\n",
        "    callbacks = get_callbacks(fold)\n",
        "\n",
        "    # Print training information\n",
        "    print('#' * 50)\n",
        "    print(f'\\tFold: {fold + 1} | Model: {CFG.preset}\\n\\tBatch Size: {CFG.batch_size * CFG.replicas} | Scheduler: {CFG.scheduler}')\n",
        "    print(f'\\tNum Train: {len(train_df)} | Num Valid: {len(valid_df)}')\n",
        "    print('#' * 50)\n",
        "\n",
        "    # Clear TensorFlow session and build the model within the strategy scope\n",
        "    K.clear_session()\n",
        "    with strategy.scope():\n",
        "        model = build_model()\n",
        "\n",
        "    # Start training the model\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=CFG.epochs,\n",
        "        validation_data=valid_ds,\n",
        "        callbacks=callbacks,\n",
        "        steps_per_epoch=int(len(train_df) / CFG.batch_size / CFG.replicas),\n",
        "    )\n",
        "\n",
        "    # Find the epoch with the best validation accuracy\n",
        "    best_epoch = np.argmax(model.history.history['val_auc'])\n",
        "    best_auc = model.history.history['val_auc'][best_epoch]\n",
        "    best_loss = model.history.history['val_loss'][best_epoch]\n",
        "\n",
        "    # Print and display best results\n",
        "    print(f'\\n{\"=\" * 17} FOLD {fold} RESULTS {\"=\" * 17}')\n",
        "    print(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST AUC   : {best_auc:.3f}\\n>>>> BEST Epoch : {best_epoch}')\n",
        "    print('=' * 50)\n",
        "\n",
        "    # Log best result on Weights and Biases (wandb) if enabled\n",
        "    if CFG.wandb:\n",
        "        log_wandb()  # Log results\n",
        "        wandb.run.finish()  # Finish the run\n",
        "#         display(ipd.IFrame(run.url, width=1080, height=720)) # show wandb dashboard\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2024-01-17T06:52:16.22966Z",
          "iopub.execute_input": "2024-01-17T06:52:16.229957Z"
        },
        "trusted": true,
        "id": "nhypzejJHZPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Log\n",
        "### [Click Here ➡️](https://wandb.ai/awsaf49/llm-fake-text) to check all the training logs in **WandB** dashboard.\n",
        "\n",
        "![image.png](attachment:4164c26d-d907-45db-9df9-93d90713d784.png)"
      ],
      "metadata": {
        "id": "eRwNR8HKHZPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧪 | Prediction"
      ],
      "metadata": {
        "id": "RI3Rk-DWHZPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the trained model on last validation data\n",
        "predictions = model.predict(\n",
        "    valid_ds,\n",
        "    batch_size=min(CFG.batch_size * CFG.replicas * 2, len(valid_df)), # max batch size = valid size\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "JfKtIaTGHZPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format predictions and true answers\n",
        "pred_answers = (predictions > 0.5).astype(int).squeeze()\n",
        "true_answers = valid_df.label.values\n",
        "\n",
        "# Check 5 Predictions\n",
        "print(\"# Predictions\\n\")\n",
        "for i in range(5):\n",
        "    row = valid_df.iloc[i]\n",
        "    text  = row.text\n",
        "    pred_answer = CFG.label2name[pred_answers[i]]\n",
        "    true_answer = CFG.label2name[true_answers[i]]\n",
        "    print(f\"❓ Text {i+1}:\\n{text[:100]} .... {text[-100:]}\\n\")\n",
        "    print(f\"✅ True: {true_answer}\\n\")\n",
        "    print(f\"🤖 Predicted: {pred_answer}\\n\")\n",
        "    print(\"-\"*90, \"\\n\")"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "6Fxep5_NHZPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✍️ | Reference\n",
        "* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)\n",
        "* [Keras NLP](https://keras.io/api/keras_nlp/)\n",
        "* [Triple Stratified KFold with TFRecords](https://www.kaggle.com/code/cdeotte/triple-stratified-kfold-with-tfrecords) by @cdeotte"
      ],
      "metadata": {
        "id": "Bigm9ZYNHZPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /kaggle/working/wandb"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "lRSUrAh6HZPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}